{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Preprocessing and pipelines"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# First, look at everything.\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input/\"]).decode(\"utf8\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/automobile/auto.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.boxplot(column='mpg', by='origin', figsize=(10,10), fontsize=10);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read 'gapminder.csv' into a DataFrame: df\ndf = pd.read_csv('../input/gapminder/gapminder.csv')\n\n# Create a boxplot of life expectancy per region\ndf.boxplot('life', 'Region', rot=60, figsize=(5,5));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating dummy variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create dummy variables: df_region\ndf_region = pd.get_dummies(df)\n\n# Print the columns of df_region\nprint(df_region.columns)\n\n# Create dummy variables with drop_first=True: df_region\ndf_region2 = pd.get_dummies(df, drop_first=True)\n\n# Print the new columns of df_region\nprint(df_region2.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_region2.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Regression with categorical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df_region2.life.values\nX = df_region2.drop('life', axis=1).values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import necessary modules\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import cross_val_score\n\n# Instantiate a ridge regressor: ridge\nridge = Ridge(alpha=.5, normalize=True)\n\n# Perform 5-fold cross-validation: ridge_cv\nridge_cv = cross_val_score(ridge, X, y, cv=5)\n\n# Print the cross-validated scores\nprint(ridge_cv)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dropping missing data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read the CSV file into a DataFrame: df\ndf = pd.read_csv('../input/house-votes-non-index/house-votes-non-index.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert '?' to NaN\ndf[df == '?'] = np.nan\n\n# Print the number of NaNs\nprint(df.isnull().sum())\n\n# Print shape of original DataFrame\nprint(\"Shape of Original DataFrame: {}\".format(df.shape))\n\n# Drop missing values and print shape of new DataFrame\ndf = df.dropna(axis=0)\n\n# Print shape of new DataFrame\nprint(\"Shape of DataFrame After Dropping All Rows with Missing Values: {}\".format(df.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When many values in your dataset are missing, if you drop them, you may end up throwing away valuable information along with the missing data."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Imputing missing data in a ML Pipeline I"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import the Imputer module\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.svm import SVC\n\n# Setup the Imputation transformer: imp\n##################\nimp = Imputer(missing_values='NaN', strategy='most_frequent', axis=0)\n\n# Instantiate the SVC classifier: clf\nclf = SVC()\n\n# Setup the pipeline with the required steps: steps\nsteps = [('imputation', imp),\n        ('SVM', clf)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Imputing missing data in a ML Pipeline II"},{"metadata":{},"cell_type":"markdown","source":"Practice this for yourself now and generate a classification report of your predictions. "},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df.party\n\nX = df.drop('party', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import necessary modules\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\n\n# Create the pipeline: pipeline\npipeline = Pipeline(steps)\n\n# Create training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y.values, test_size=.3, random_state=42)\n\n# Fit the pipeline to the train set\npipeline.fit(X_train, y_train)\n\n# Predict the labels of the test set\ny_pred = pipeline.predict(X_test)\n\n# Compute metrics\nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Your pipeline has performed imputation as well as classification!"},{"metadata":{},"cell_type":"markdown","source":"### Centering and scaling your data"},{"metadata":{"trusted":true},"cell_type":"code","source":"w = pd.read_csv('../input/white-wine/white-wine.csv')\nw.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = w.drop('quality', axis=1).values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import scale\nfrom sklearn.preprocessing import scale\n\n# Scale the features: X_scaled\nX_scaled = scale(X)\n\n# Print the mean and standard deviation of the unscaled features\nprint(\"Mean of Unscaled Features: {}\".format(np.mean(X))) \nprint(\"Standard Deviation of Unscaled Features: {}\".format(np.std(X)))\n\n# Print the mean and standard deviation of the scaled features\nprint(\"Mean of Scaled Features: {}\".format(np.mean(X_scaled))) \nprint(\"Standard Deviation of Scaled Features: {}\".format(np.std(X_scaled)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Centering and scaling in a pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = w.quality.apply(lambda x: True if x < 6 else False) # or without .values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y[:20]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import the necessary modules\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Setup the pipeline steps: steps\nsteps = [('scaler', StandardScaler()),\n        ('knn', KNeighborsClassifier())]\n        \n# Create the pipeline: pipeline\npipeline = Pipeline(steps)\n\n# Create train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=.3, random_state=42)\n\n# Fit the pipeline to the training set: knn_scaled\nknn_scaled = pipeline.fit(X_train, y_train)\n\n# Instantiate and fit a k-NN classifier to the unscaled data\nknn_unscaled = KNeighborsClassifier().fit(X_train, y_train)\n\n# Compute and print metrics\nprint('Accuracy with Scaling: {}'.format(pipeline.score(X_test, y_test)))\nprint('Accuracy without Scaling: {}'.format(knn_unscaled.score(X_test, y_test)))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Bringing it all together I: Pipeline for classification"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\n# Setup the pipeline\nsteps = [('scaler', StandardScaler()),\n         ('SVM', SVC())]\n\npipeline = Pipeline(steps)\n\n# Specify the hyperparameter space\nparameters = {'SVM__C':[1, 10, 100],\n              'SVM__gamma':[0.1, 0.01]}\n\n# Create train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=21)\n\n# Instantiate the GridSearchCV object: cv\ncv = GridSearchCV(pipeline, parameters, cv=3)\n\n# Fit to the training set\ncv.fit(X_train, y_train)\n\n# Predict the labels of the test set: y_pred\ny_pred = cv.predict(X_test)\n\n# Compute and print metrics\nprint(\"Accuracy: {}\".format(cv.score(X_test, y_test)))\nprint(classification_report(y_test, y_pred))\nprint(\"Tuned Model Parameters: {}\".format(cv.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import recall_score\n\nrecall_score(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test[:10].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Bringing it all together II: Pipeline for regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/gapminder/gapminder.csv')\n\n# Create arrays for features and target variable\ny = df.life\nX = df.drop(['life', 'Region'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import ElasticNet\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Setup the pipeline steps: steps\nsteps = [('imputation', Imputer(missing_values='NaN', strategy='mean', axis=0)),\n         ('scaler', StandardScaler()),\n         ('elasticnet', ElasticNet())]\n\n# Create the pipeline: pipeline \npipeline = Pipeline(steps)\n\n# Specify the hyperparameter space\nparameters = {'elasticnet__l1_ratio':np.linspace(0,1,30)}\n\n# Create train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4, random_state=42)\n\n# Create the GridSearchCV object: gm_cv\ngm_cv = GridSearchCV(pipeline, parameters, cv=3)\n\n# Fit to the training set\ngm_cv.fit(X_train,y_train)\n\n# Compute and print the metrics\nr2 = gm_cv.score(X_test, y_test)\nprint(\"Tuned ElasticNet Alpha: {}\".format(gm_cv.best_params_))\nprint(\"Tuned ElasticNet R squared: {}\".format(r2))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}