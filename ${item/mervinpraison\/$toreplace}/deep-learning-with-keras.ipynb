{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Coding the forward propagation algorithm"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<br><br>\n![](https://raw.githubusercontent.com/MervinPraison/datacamp-1/master/Deep%20Learning/images/1_4.png)\n<br><br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"weights = { 'node_0': np.array([2,4]),\n            'node_1': np.array([4, -5]),\n            'output': np.array([2,7])}\n\ninput_data = np.array([3,5])\n\n# Calculate node 0 value: node_0_value\nnode_0_value = (weights['node_0'] * input_data).sum()\n\n# Calculate node 1 value: node_1_value\nnode_1_value = (weights['node_1'] * input_data).sum()\n\n# Put node values into array: hidden_layer_outputs\nhidden_layer_outputs = np.array([node_0_value, node_1_value])\n\n# Calculate output: output\noutput = (weights['output'] * hidden_layer_outputs).sum()\n\n# Print output\nprint(output)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The Rectified Linear Activation Function"},{"metadata":{"trusted":true},"cell_type":"code","source":"def relu(input):\n    '''Define your relu activation function here'''\n    # Calculate the value for the output of the relu function: output\n    output = max(input, 0)\n\n    # Return the value just calculated\n    return(output)\n\n# Calculate node 0 value: node_0_output\nnode_0_input = (input_data * weights['node_0']).sum()\nnode_0_output = relu(node_0_input)\n\n# Calculate node 1 value: node_1_output\nnode_1_input = (input_data * weights['node_1']).sum()\nnode_1_output = relu(node_1_input)\n\n# Put node values into array: hidden_layer_outputs\nhidden_layer_outputs = np.array([node_0_output, node_1_output])\n\n# Calculate model output (do not apply relu)\nmodel_output = (hidden_layer_outputs * weights['output']).sum()\n\n# Print model output\nprint(model_output)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"relu(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"relu(-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define predict_with_network()\ndef predict_with_network(input_data_row, weights):\n\n    # Calculate node 0 value\n    node_0_input = (input_data_row * weights['node_0']).sum()\n    node_0_output = relu(node_0_input)\n\n    # Calculate node 1 value\n    node_1_input = (input_data_row * weights['node_1']).sum()\n    node_1_output = relu(node_1_input)\n\n    # Put node values into array: hidden_layer_outputs\n    hidden_layer_outputs = np.array([node_0_output, node_1_output])\n    \n    # Calculate model output\n    input_to_final_layer = (hidden_layer_outputs * weights['output']).sum()\n    model_output = relu(input_to_final_layer)\n    \n    # Return model output\n    return(model_output)\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weights = {'node_0': np.array([2, 4]), 'node_1': np.array([ 4, -5]), 'output': np.array([2, 7])}\ninput_data = [np.array([3, 5]), np.array([ 1, -1]), np.array([0, 0]), np.array([8, 4])]\n\n# Create empty list to store prediction results\nresults = []\nfor input_data_row in input_data:\n    # Append prediction to results\n    results.append(predict_with_network(input_data_row, weights))\n\n# Print results\nprint(results)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Multi-layer neural networks"},{"metadata":{"trusted":true},"cell_type":"code","source":"weights = {'node_0_0': np.array([2, 4]),\n 'node_0_1': np.array([ 4, -5]),\n 'node_1_0': np.array([-1,  2]),\n 'node_1_1': np.array([1, 2]),\n 'output': np.array([2, 7])}\n\ninput_data = np.array([3, 5])\n\ndef predict_with_network(input_data):\n    # Calculate node 0 in the first hidden layer\n    node_0_0_input = (input_data * weights['node_0_0']).sum()\n    node_0_0_output = relu(node_0_0_input)\n\n    # Calculate node 1 in the first hidden layer\n    node_0_1_input = (input_data * weights['node_0_1']).sum()\n    node_0_1_output = relu(node_0_1_input)\n\n    # Put node values into array: hidden_0_outputs\n    hidden_0_outputs = np.array([node_0_0_output, node_0_1_output])\n    \n    # Calculate node 0 in the second hidden layer\n    node_1_0_input = (hidden_0_outputs * weights['node_1_0']).sum()\n    node_1_0_output = relu(node_1_0_input)\n\n    # Calculate node 1 in the second hidden layer\n    node_1_1_input = (hidden_0_outputs * weights['node_1_1']).sum()\n    node_1_1_output = relu(node_1_1_input)\n\n    # Put node values into array: hidden_1_outputs\n    hidden_1_outputs = np.array([node_1_0_output, node_1_1_output])\n\n    # Calculate model output: model_output\n    model_output = (hidden_1_outputs * weights['output']).sum()\n    \n    # Return model_output\n    return(model_output)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output = predict_with_network(input_data)\noutput","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Q: Which layers of a model capture more complex or \"higher level\" interactions?\n\nThe last layers capture the most complex interactions.\n"},{"metadata":{},"cell_type":"markdown","source":"## Coding how weight changes affect accuracy"},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict_with_network(input_data, weights):\n    # Calculate node 0 in the first hidden layer\n    node_0_0_input = (input_data * weights['node_0']).sum()\n    node_0_0_output = relu(node_0_0_input)\n\n    # Calculate node 1 in the first hidden layer\n    node_0_1_input = (input_data * weights['node_1']).sum()\n    node_0_1_output = relu(node_0_1_input)\n\n    # Put node values into array: hidden_0_outputs\n    hidden_0_outputs = np.array([node_0_0_output, node_0_1_output])\n\n    # Calculate model output: model_output\n    model_output = (hidden_0_outputs * weights['output']).sum()\n    \n    # Return model_output\n    return(model_output)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<br><br>\n![](https://raw.githubusercontent.com/MervinPraison/datacamp-1/master/Deep%20Learning/images/1_5.png)\n<br><br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# The data point you will make a prediction for\ninput_data = np.array([0, 3])\n\n# Sample weights\nweights_0 = {'node_0': [2, 1],\n             'node_1': [1, 2],\n             'output': [1, 1]\n            }\n\n# The actual target value, used to calculate the error\ntarget_actual = 3\n\n# Make prediction using original weights\nmodel_output_0 = predict_with_network(input_data, weights_0)\n\n# Calculate error: error_0\nerror_0 = model_output_0 - target_actual\n\n# Create weights that cause the network to make perfect prediction (3): weights_1\nweights_1 = {'node_0': [2, 1],\n             # changed node_1 weight as [1,0]\n             'node_1': [1, 0],\n             'output': [1,1]\n            }\n\n# Make prediction using new weights: model_output_1\nmodel_output_1 = predict_with_network(input_data, weights_1)\n\n# Calculate error: error_1\nerror_1 = model_output_1 - target_actual\n\n# Print error_0 and error_1\nprint(error_0)\nprint(error_1)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Scaling up to multiple data points"},{"metadata":{"trusted":true},"cell_type":"code","source":"input_data = [np.array([0, 3]), np.array([1, 2]), np.array([-1, -2]), np.array([4, 0])]\ntarget_actuals = [1, 3, 5, 7]\n\nweights_0 ={'node_0': np.array([2, 1]), 'node_1': np.array([1, 2]), 'output': np.array([1, 1])}\nweights_1 = {'node_0': np.array([2, 1]), 'node_1': np.array([1. , 1.5]), 'output': np.array([1. , 1.5])}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\n# Create model_output_0 \nmodel_output_0 = []\n# Create model_output_0\nmodel_output_1 = []\n\n# Loop over input_data\nfor row in input_data:\n    # Append prediction to model_output_0\n    model_output_0.append(predict_with_network(row, weights_0))\n    \n    # Append prediction to model_output_1\n    model_output_1.append(predict_with_network(row, weights_1))\n\n# Calculate the mean squared error for model_output_0: mse_0\nmse_0 = mean_squared_error(target_actuals, model_output_0)\n\n# Calculate the mean squared error for model_output_1: mse_1\nmse_1 = mean_squared_error(target_actuals, model_output_1)\n\n# Print mse_0 and mse_1\nprint(\"Mean squared error with weights_0: %f\" %mse_0)\nprint(\"Mean squared error with weights_1: %f\" %mse_1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_output_0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_output_1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Calculating slopes"},{"metadata":{},"cell_type":"markdown","source":"When plotting the mean-squared error loss function against predictions, the slope is `2 * x * (y-xb)`, or `2 * input_data * error`."},{"metadata":{"trusted":true},"cell_type":"code","source":"input_data = np.array([1, 2, 3])\nweights = np.array([0, 2, 1])\ntarget = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate the predictions: preds\npreds = (weights * input_data).sum()\n\n# Calculate the error: error\nerror = preds - target\n\n# Calculate the slope: slope\nslope = 2 * input_data * error\n\n# Print the slope\nprint(slope)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You've just calculated the slopes you need. Now it's time to use those slopes to improve your model.\n\n## Improving model weights"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set the learning rate: learning_rate\nlearning_rate = 0.01\n\n# Calculate the predictions: preds\npreds = (weights * input_data).sum()\n\n# Calculate the error: error\nerror = preds - target\n\n# Calculate the slope: slope\nslope = 2 * input_data * error\n\n# Update the weights: weights_updated\nweights_updated = weights - ( learning_rate * slope)\n\n# Get updated predictions: preds_updated\npreds_updated = (weights_updated * input_data).sum()\n\n# Calculate updated error: error_updated\nerror_updated = preds_updated - target\n\n# Print the original error\nprint(error)\n\n# Print the updated error\nprint(error_updated)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Making multiple updates to weights"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_data = np.array([1, 2, 3])\nweights = np.array([-0.49929916,  1.00140168, -0.49789747])\ntarget = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_slope(input_data, target, weights):\n    # Calculate the predictions: preds\n    preds = (weights * input_data).sum()\n\n    # Calculate the error: error\n    error = preds - target\n\n    # Calculate the slope: slope\n    slope = 2 * input_data * error\n    \n    return slope","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_mse(input_data, target, weights_updated):\n\n    # Get updated predictions: preds_updated\n    preds_updated = (weights_updated * input_data).sum()\n\n    # Calculate updated error: error_updated\n    error_updated = preds_updated - target\n    \n    return error_updated","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_updates = 20\nmse_hist = []\n\n# Iterate over the number of updates\nfor i in range(n_updates):\n    \n    # Calculate the slope: slope\n    slope = get_slope(input_data, target, weights)\n    \n    # Update the weights: weights\n    weights = weights - 0.01 * slope\n    \n    # Calculate mse with new weights: mse\n    mse = get_mse(input_data, target, weights)\n    \n    # Append the mse to mse_hist\n    mse_hist.append(mse)\n\n# Plot the mse history\nplt.plot(mse_hist)\nplt.xlabel('Iterations')\nplt.ylabel('Mean Squared Error')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating a keras model\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import necessary modules\nimport pandas as pd\nimport keras\nfrom keras.layers import Dense\nfrom keras.models import Sequential\n\n# Save the number of columns in predictors: n_cols\ndf = pd.read_csv('../input/hourly-wages/hourly_wages.csv')\n#print(df.head())\npredictors = df.drop(columns=['wage_per_hour'], axis=1).values\ntarget = df['wage_per_hour'].values\nn_cols = predictors.shape[1]\n\n# Set up the model: model\nmodel = Sequential()\n\n# Add the first layer\nmodel.add(Dense(50, activation='relu', input_shape=(n_cols,)))\n\n# Add the second layer\nmodel.add(Dense(32, activation='relu'))\n\n# Add the output layer\nmodel.add(Dense(1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Compiling and fitting a model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compile the model\nmodel.compile(optimizer='adam', loss='mean_squared_error')\n\n# Verify that model contains information from compiling\nprint(\"Loss function: \" + model.loss)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit the model\nmodel.fit(predictors, target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import necessary modules\nimport pandas as pd\nimport keras\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom keras.utils import to_categorical\n\ndf = pd.read_csv('../input/titanic/titanic_all_numeric.csv')\nprint(df.head())\npredictors = df.drop(columns=['survived'], axis=1).values\n#target = df['wage_per_hour'].values\nn_cols = predictors.shape[1]\n\n# Convert the target to categorical: target\ntarget = to_categorical(df.survived)\n\n# Set up the model\nmodel = Sequential()\n\n# Add the first layer\nmodel.add(Dense(32, activation='relu', input_shape=(n_cols,)))\n\n# Add the output layer\nmodel.add(Dense(2, activation='softmax'))\n\n# Compile the model\nmodel.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Fit the model\nmodel.fit(predictors, target)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Making Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Specify, compile, and fit the model\n# Convert the target to categorical: target\n#target = df['survived']\npred_data = np.array([[2,34.0,0,0,13.0,1,False,0,0,1] , [2,31.0,1,1,26.25,0,False,0,0,1] , [1,11.0,1,2,120.0,1,False,0,0,1] , [3,0.42,0,1,8.5167,1,False,1,0,0] , [3,27.0,0,0,6.975,1,False,0,0,1] , [3,31.0,0,0,7.775,1,False,0,0,1] , [1,39.0,0,0,0.0,1,False,0,0,1] , [3,18.0,0,0,7.775,0,False,0,0,1] , [2,39.0,0,0,13.0,1,False,0,0,1] , [1,33.0,1,0,53.1,0,False,0,0,1] , [3,26.0,0,0,7.8875,1,False,0,0,1] , [3,39.0,0,0,24.15,1,False,0,0,1] , [2,35.0,0,0,10.5,1,False,0,0,1] , [3,6.0,4,2,31.275,0,False,0,0,1] , [3,30.5,0,0,8.05,1,False,0,0,1] , [1,29.69911764705882,0,0,0.0,1,True,0,0,1] , [3,23.0,0,0,7.925,0,False,0,0,1] , [2,31.0,1,1,37.0042,1,False,1,0,0] , [3,43.0,0,0,6.45,1,False,0,0,1] , [3,10.0,3,2,27.9,1,False,0,0,1] , [1,52.0,1,1,93.5,0,False,0,0,1] , [3,27.0,0,0,8.6625,1,False,0,0,1] , [1,38.0,0,0,0.0,1,False,0,0,1] , [3,27.0,0,1,12.475,0,False,0,0,1] , [3,2.0,4,1,39.6875,1,False,0,0,1] , [3,29.69911764705882,0,0,6.95,1,True,0,1,0] , [3,29.69911764705882,0,0,56.4958,1,True,0,0,1] , [2,1.0,0,2,37.0042,1,False,1,0,0] , [3,29.69911764705882,0,0,7.75,1,True,0,1,0] , [1,62.0,0,0,80.0,0,False,0,0,0] , [3,15.0,1,0,14.4542,0,False,1,0,0] , [2,0.83,1,1,18.75,1,False,0,0,1] , [3,29.69911764705882,0,0,7.2292,1,True,1,0,0] , [3,23.0,0,0,7.8542,1,False,0,0,1] , [3,18.0,0,0,8.3,1,False,0,0,1] , [1,39.0,1,1,83.1583,0,False,1,0,0] , [3,21.0,0,0,8.6625,1,False,0,0,1] , [3,29.69911764705882,0,0,8.05,1,True,0,0,1] , [3,32.0,0,0,56.4958,1,False,0,0,1] , [1,29.69911764705882,0,0,29.7,1,True,1,0,0] , [3,20.0,0,0,7.925,1,False,0,0,1] , [2,16.0,0,0,10.5,1,False,0,0,1] , [1,30.0,0,0,31.0,0,False,1,0,0] , [3,34.5,0,0,6.4375,1,False,1,0,0] , [3,17.0,0,0,8.6625,1,False,0,0,1] , [3,42.0,0,0,7.55,1,False,0,0,1] , [3,29.69911764705882,8,2,69.55,1,True,0,0,1] , [3,35.0,0,0,7.8958,1,False,1,0,0] , [2,28.0,0,1,33.0,1,False,0,0,1] , [1,29.69911764705882,1,0,89.1042,0,True,1,0,0] , [3,4.0,4,2,31.275,1,False,0,0,1] , [3,74.0,0,0,7.775,1,False,0,0,1] , [3,9.0,1,1,15.2458,0,False,1,0,0] , [1,16.0,0,1,39.4,0,False,0,0,1] , [2,44.0,1,0,26.0,0,False,0,0,1] , [3,18.0,0,1,9.35,0,False,0,0,1] , [1,45.0,1,1,164.8667,0,False,0,0,1] , [1,51.0,0,0,26.55,1,False,0,0,1] , [3,24.0,0,3,19.2583,0,False,1,0,0] , [3,29.69911764705882,0,0,7.2292,1,True,1,0,0] , [3,41.0,2,0,14.1083,1,False,0,0,1] , [2,21.0,1,0,11.5,1,False,0,0,1] , [1,48.0,0,0,25.9292,0,False,0,0,1] , [3,29.69911764705882,8,2,69.55,0,True,0,0,1] , [2,24.0,0,0,13.0,1,False,0,0,1] , [2,42.0,0,0,13.0,0,False,0,0,1] , [2,27.0,1,0,13.8583,0,False,1,0,0] , [1,31.0,0,0,50.4958,1,False,0,0,1] , [3,29.69911764705882,0,0,9.5,1,True,0,0,1] , [3,4.0,1,1,11.1333,1,False,0,0,1] , [3,26.0,0,0,7.8958,1,False,0,0,1] , [1,47.0,1,1,52.5542,0,False,0,0,1] , [1,33.0,0,0,5.0,1,False,0,0,1] , [3,47.0,0,0,9.0,1,False,0,0,1] , [2,28.0,1,0,24.0,0,False,1,0,0] , [3,15.0,0,0,7.225,0,False,1,0,0] , [3,20.0,0,0,9.8458,1,False,0,0,1] , [3,19.0,0,0,7.8958,1,False,0,0,1] , [3,29.69911764705882,0,0,7.8958,1,True,0,0,1] , [1,56.0,0,1,83.1583,0,False,1,0,0] , [2,25.0,0,1,26.0,0,False,0,0,1] , [3,33.0,0,0,7.8958,1,False,0,0,1] , [3,22.0,0,0,10.5167,0,False,0,0,1] , [2,28.0,0,0,10.5,1,False,0,0,1] , [3,25.0,0,0,7.05,1,False,0,0,1] , [3,39.0,0,5,29.125,0,False,0,1,0] , [2,27.0,0,0,13.0,1,False,0,0,1] , [1,19.0,0,0,30.0,0,False,0,0,1] , [3,29.69911764705882,1,2,23.45,0,True,0,0,1] , [1,26.0,0,0,30.0,1,False,1,0,0] , [3,32.0,0,0,7.75,1,False,0,1,0]])\n#print(type(pred_data))\npredictors = df.drop(columns=['survived'], axis=1)\n#target = df['wage_per_hour'].values\nn_cols = predictors.shape[1]\n# Convert the target to categorical: target\ntarget = to_categorical(df.survived)\n#print(target)\n\nmodel = Sequential()\nmodel.add(Dense(32, activation='relu', input_shape = (n_cols,)))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(optimizer='sgd', \n              loss='categorical_crossentropy', \n              metrics=['accuracy'])\nmodel.fit(predictors, target)\n\n# Calculate predictions: predictions\npredictions = model.predict(pred_data)\n\n# Calculate predicted probability of survival: predicted_prob_true\npredicted_prob_true = predictions[:,1]\n\n# print predicted_prob_true\nprint(predicted_prob_true)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Changing optimization parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_new_model():\n    model = Sequential()\n    model.add(Dense(100, activation='relu', input_shape = (n_cols,)))\n    model.add(Dense(100, activation='relu'))\n    model.add(Dense(2, activation='softmax'))\n    return(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import the SGD optimizer\nfrom keras.optimizers import SGD\n\n# Create list of learning rates: lr_to_test\nlr_to_test = [.000001, 0.01, 1]\n\n# Loop over learning rates\nfor lr in lr_to_test:\n    print('\\n\\nTesting model with learning rate: %f\\n'%lr )\n    \n    # Build new model to test, unaffected by previous models\n    model = get_new_model()\n    \n    # Create SGD optimizer with specified learning rate: my_optimizer\n    my_optimizer = SGD(lr=lr)\n    \n    # Compile the model\n    model.compile(optimizer=my_optimizer, loss='categorical_crossentropy')\n    \n    # Fit the model\n    model.fit(predictors, target)\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model validation\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save the number of columns in predictors: n_cols\nn_cols = predictors.shape[1]\ninput_shape = (n_cols,)\n\n# Specify the model\nmodel = Sequential()\nmodel.add(Dense(100, activation='relu', input_shape = input_shape))\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dense(2, activation='softmax'))\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Fit the model\nhist = model.fit(predictors, target, validation_split=.3)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Early stopping: Optimizing the optimization"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import EarlyStopping\nfrom keras.callbacks import EarlyStopping\n\n# Save the number of columns in predictors: n_cols\nn_cols = predictors.shape[1]\ninput_shape = (n_cols,)\n\n# Specify the model\nmodel = Sequential()\nmodel.add(Dense(100, activation='relu', input_shape = input_shape))\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dense(2, activation='softmax'))\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Define early_stopping_monitor\nearly_stopping_monitor = EarlyStopping(patience=2)\n\n# Fit the model\nmodel.fit(predictors, target, epochs=30, validation_split=.3, callbacks=[early_stopping_monitor])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Experimenting with wider networks"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define early_stopping_monitor\nearly_stopping_monitor = EarlyStopping(patience=2)\n\n# Save the number of columns in predictors: n_cols\nn_cols = predictors.shape[1]\ninput_shape = (n_cols,)\n\n# Specify the model\nmodel_1 = Sequential()\nmodel_1.add(Dense(100, activation='relu', input_shape = input_shape))\nmodel_1.add(Dense(100, activation='relu'))\nmodel_1.add(Dense(2, activation='softmax'))\n\n# Compile the model\nmodel_1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nmodel_1.fit(predictors, target, epochs=30, validation_split=.3, callbacks=[early_stopping_monitor])\n# Create the new model: model_2\nmodel_2 = Sequential()\n\n# Add the first and second layers\nmodel_2.add(Dense(100, activation='relu', input_shape=input_shape))\nmodel_2.add(Dense(100, activation='relu', input_shape=input_shape))\n\n# Add the output layer\nmodel_2.add(Dense(2, activation='softmax'))\n\n# Compile model_2\nmodel_2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Fit model_1\nmodel_1_training = model_1.fit(predictors, target, epochs=15, validation_split=0.2, callbacks=[early_stopping_monitor], verbose=False)\n\n# Fit model_2\nmodel_2_training = model_2.fit(predictors, target, epochs=15, validation_split=0.2, callbacks=[early_stopping_monitor], verbose=False)\n\n# Create the plot\nplt.plot(model_1_training.history['val_loss'], 'r', model_2_training.history['val_loss'], 'b')\nplt.xlabel('Epochs')\nplt.ylabel('Validation score')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Adding layers to a network"},{"metadata":{"trusted":true},"cell_type":"code","source":"# The input shape to use in the first hidden layer\ninput_shape = (n_cols,)\n\n# Create the new model: model_2\nmodel_2 = Sequential()\n\n# Add the first, second, and third hidden layers\nmodel_2.add(Dense(50, activation='relu', input_shape=input_shape))\nmodel_2.add(Dense(50, activation='relu'))\nmodel_2.add(Dense(50, activation='relu'))\n\n# Add the output layer\nmodel_2.add(Dense(2, activation='softmax'))\n\n# Compile model_2\nmodel_2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Fit model 1\nmodel_1_training = model_1.fit(predictors, target, epochs=20, validation_split=0.4, callbacks=[early_stopping_monitor], verbose=False)\n\n# Fit model 2\nmodel_2_training = model_2.fit(predictors, target, epochs=20, validation_split=0.4, callbacks=[early_stopping_monitor], verbose=False)\n\n# Create the plot\nplt.plot(model_1_training.history['val_loss'], 'r', model_2_training.history['val_loss'], 'b')\nplt.xlabel('Epochs')\nplt.ylabel('Validation score')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model with the lower loss value is the better model"},{"metadata":{},"cell_type":"markdown","source":"## Building your own digit recognition model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import necessary modules\nimport pandas as pd\nimport keras\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom keras.utils import to_categorical\n\n\ndf = pd.read_csv('../input/mnist-data/mnist.csv')\nprint(df.head())\nX = df.drop(df.columns[0], axis=1).values\n# Convert the target to categorical: target\ny = to_categorical(df.iloc[:,0].values)\nprint(X.shape)\nprint(y.shape)\n\n#n_cols = predictors.shape[1]\n\n\n#target = to_categorical(df.survived)\n\n# Create the model: model\nmodel = Sequential()\n\n# Add the first hidden layer\nmodel.add(Dense(50, activation='relu', input_shape=(X[0].shape[0],)))\n\n# Add the second hidden layer\nmodel.add(Dense(50, activation='relu'))\n\n# Add the output layer\nmodel.add(Dense(10, activation='softmax'))\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Fit the model\nmodel.fit(X, y, validation_split=.3)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You should see better than 90% accuracy recognizing handwritten digits, even while using a small training set of only 1750 images!"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}