{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Regular expressions & word tokenization\n "},{"metadata":{},"cell_type":"markdown","source":"## Introduction to regular expressions "},{"metadata":{},"cell_type":"markdown","source":"Regular expressions: strings with a special syntax; allow us to match patterns in other strings"},{"metadata":{"ExecuteTime":{"end_time":"2019-06-21T03:13:54.828461Z","start_time":"2019-06-21T03:13:54.818404Z"},"trusted":true},"cell_type":"code","source":"import re","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-06-21T03:13:54.865828Z","start_time":"2019-06-21T03:13:54.836800Z"},"trusted":true},"cell_type":"code","source":"# match a pattern with a string\nre.match('abc', 'abcdef')","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-06-21T03:13:54.884642Z","start_time":"2019-06-21T03:13:54.872925Z"},"trusted":true},"cell_type":"code","source":"# use special patterns that regex understand\nword_regex = '\\w+' # match a word\nre.match(word_regex, 'hi there!') # match the first word it finds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Common Regex patterns: \\w+ (word), \\d (digit), \\s (space), .* (wildcard - any letter or symbol), + or * (greedy match - repetition of letter/symbol), \\S (anything not space), [a-z] (lowercase group)"},{"metadata":{},"cell_type":"markdown","source":"re Module: split, findall, search, match"},{"metadata":{},"cell_type":"markdown","source":"Syntax: pattern first, string second. May return an iterator, string, or match object."},{"metadata":{"ExecuteTime":{"end_time":"2019-06-21T03:13:54.906809Z","start_time":"2019-06-21T03:13:54.896585Z"},"trusted":true},"cell_type":"code","source":"my_string = \"Let's write RegEx!\"","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-06-21T03:13:54.924244Z","start_time":"2019-06-21T03:13:54.911615Z"},"trusted":true},"cell_type":"code","source":"# Write a pattern to match sentence endings: sentence_endings\nsentence_endings = r\"[.?!]\"\n\n# Split my_string on sentence endings and print the result\nprint(re.split(sentence_endings, my_string))\n\n# Find all capitalized words in my_string and print the result\ncapitalized_words = r\"[A-Z]\\w+\"\nprint(re.findall(capitalized_words, my_string))\n\n# Split my_string on spaces and print the result\nspaces = r\"\\s+\"\nprint(re.split(spaces, my_string))\n\n# Find all digits in my_string and print the result\ndigits = r\"\\d+\"\nprint(re.findall(digits, my_string))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Introduction to tokenization "},{"metadata":{},"cell_type":"markdown","source":"Tokenization: a process of turning a string or document into tokens (smaller chunks); one step in preparing a text for NLP, many different theories and rules; you can create your own rules using Regex. Examples: breaking out words or sentences, separating punctuation, separating all hashtags in a tweet."},{"metadata":{},"cell_type":"markdown","source":"nltk (natural language toolkit) library."},{"metadata":{"ExecuteTime":{"end_time":"2019-06-21T03:13:58.395094Z","start_time":"2019-06-21T03:13:54.930808Z"},"trusted":true},"cell_type":"code","source":"from nltk.tokenize import word_tokenize\n\nword_tokenize('Hi there!')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tokenization makes it easier to map part of speech, match common words, and remove unwanted tokens."},{"metadata":{},"cell_type":"markdown","source":"Other nltk tokenizers: sent_tokenize, regexp_tokenize, TweetTokenizer."},{"metadata":{},"cell_type":"markdown","source":"Different between re.search() and re.match():"},{"metadata":{"ExecuteTime":{"end_time":"2019-06-21T03:13:58.408484Z","start_time":"2019-06-21T03:13:58.399287Z"},"trusted":true},"cell_type":"code","source":"re.match('abc', 'abcde')","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-06-21T03:13:58.425456Z","start_time":"2019-06-21T03:13:58.412713Z"},"trusted":true},"cell_type":"code","source":"re.search('abc', 'abcde')","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-06-21T03:13:58.438416Z","start_time":"2019-06-21T03:13:58.430121Z"},"trusted":true},"cell_type":"code","source":"re.match('cd', 'abcde')","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-06-21T03:13:58.460701Z","start_time":"2019-06-21T03:13:58.448206Z"},"trusted":true},"cell_type":"code","source":"re.search('cd', 'abcde')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Advanced tokenization with NLTK and regex "},{"metadata":{},"cell_type":"markdown","source":"### Regex groups using OR \"|\" "},{"metadata":{},"cell_type":"markdown","source":"Define a group using (), define explicit character ranges using []."},{"metadata":{"ExecuteTime":{"end_time":"2019-06-21T03:13:58.472030Z","start_time":"2019-06-21T03:13:58.464535Z"},"trusted":true},"cell_type":"code","source":"match_digits_and_words = ('(\\d+|\\w+)')","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-06-21T03:13:58.500347Z","start_time":"2019-06-21T03:13:58.483957Z"},"trusted":true},"cell_type":"code","source":"re.findall(match_digits_and_words, 'He has 11 cats.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Other patterns: [A-Za-z]+ (upper and lowercase English alphabet), [0-9], [A-Za-z\\-\\.]+ (all English alphabet, - and .), (a-z) (a - and z), (\\s+|,) (space or a comma)"},{"metadata":{"ExecuteTime":{"end_time":"2019-06-21T03:13:58.524810Z","start_time":"2019-06-21T03:13:58.511510Z"},"trusted":true},"cell_type":"code","source":"# character range with re.match()\nmy_str = 'match lowercase spaces nums like 12, but no commas'\nre.match('[a-z0-9 ]+', my_str)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Charting word length with nltk "},{"metadata":{"ExecuteTime":{"end_time":"2019-06-21T03:13:59.043725Z","start_time":"2019-06-21T03:13:58.529784Z"},"trusted":true},"cell_type":"code","source":"from matplotlib import pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-06-21T03:13:59.222997Z","start_time":"2019-06-21T03:13:59.046282Z"},"trusted":true},"cell_type":"code","source":"plt.hist([1, 5, 5, 7, 7, 7, 9])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Combining NLP data extraction with plotting"},{"metadata":{"ExecuteTime":{"end_time":"2019-06-21T03:13:59.232310Z","start_time":"2019-06-21T03:13:59.226975Z"},"trusted":true},"cell_type":"code","source":"words = word_tokenize('This is a pretty cool tool!')","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-06-21T03:13:59.396768Z","start_time":"2019-06-21T03:13:59.238526Z"},"trusted":true},"cell_type":"code","source":"# use list comprehension to transform it to a list of lengths\nword_lengths = [len(w) for w in words]\nplt.hist(word_lengths)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, we have the majority of 4-letter words in the sentence."},{"metadata":{},"cell_type":"markdown","source":"# Simple topic identification "},{"metadata":{},"cell_type":"markdown","source":"## Word counts with bag-of-words"},{"metadata":{},"cell_type":"markdown","source":"Basic method for finding topics in a text. Need to first create tokens using tokenization, and then count up all the tokens. The more frequent a word/token is, the more important it might be."},{"metadata":{"ExecuteTime":{"end_time":"2019-06-21T03:13:59.411822Z","start_time":"2019-06-21T03:13:59.401561Z"},"trusted":true},"cell_type":"code","source":"from nltk.tokenize import word_tokenize\nfrom collections import Counter\n\ncounter = Counter(word_tokenize(\"\"\"The cat is in the box. The cat likes the box.\nThe box is over the cat.\"\"\"))","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-06-21T03:13:59.430895Z","start_time":"2019-06-21T03:13:59.419182Z"},"trusted":true},"cell_type":"code","source":"counter.most_common(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Simple text preprocessing"},{"metadata":{},"cell_type":"markdown","source":"Why preprocess: helps make for better input data. Examples: tokenization to create a bag of words, lowercasing words."},{"metadata":{},"cell_type":"markdown","source":"Lemmatization/Stemming: shorten words to their root stems."},{"metadata":{},"cell_type":"markdown","source":"Removing stop words (and, the), punctuation, or unwanted tokens"},{"metadata":{"ExecuteTime":{"end_time":"2019-06-21T03:13:59.463521Z","start_time":"2019-06-21T03:13:59.437965Z"},"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\n\ntext = \"\"\"The cat is in the box. The cat likes the box.\nThe box is over the cat.\"\"\"\n\n# take each token of the lowercase text, if only it contains alphabetical characters\ntokens = [w for w in word_tokenize(text.lower()) if w.isalpha()]\n\n# remove stopwords\nno_stops = [t for t in tokens if t not in stopwords.words('english')]","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-06-21T03:13:59.473763Z","start_time":"2019-06-21T03:13:59.465926Z"},"trusted":true},"cell_type":"code","source":"Counter(no_stops).most_common(2)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-06-21T03:14:01.282464Z","start_time":"2019-06-21T03:13:59.481106Z"},"trusted":true},"cell_type":"code","source":"# Import WordNetLemmatizer\nfrom nltk.stem import WordNetLemmatizer\n\n# Retain alphabetic words: alpha_only\nalpha_only = [t for t in text if t.isalpha()]\n\n# Remove all stop words: no_stops\nno_stops = [t for t in alpha_only if t not in stopwords.words('english')]\n\n# Instantiate the WordNetLemmatizer\nwordnet_lemmatizer = WordNetLemmatizer()\n\n# Lemmatize all tokens into a new list: lemmatized\nlemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n\n# Create the bag-of-words: bow\nbow = Counter(lemmatized)\n\n# Print the 10 most common tokens\nprint(bow.most_common(10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Introduction to gensim"},{"metadata":{},"cell_type":"markdown","source":"Popular open-source NLP library; uses top academic models to perform complex tasks: building document or word vectors, performing topic identification and document comparison"},{"metadata":{},"cell_type":"markdown","source":"What is a word vector?"},{"metadata":{},"cell_type":"markdown","source":"Gensim example: using LDA..."},{"metadata":{},"cell_type":"markdown","source":"Creating a gensim dictionary:"},{"metadata":{"ExecuteTime":{"end_time":"2019-06-21T03:14:02.372536Z","start_time":"2019-06-21T03:14:01.288177Z"},"trusted":true},"cell_type":"code","source":"from gensim.corpora.dictionary import Dictionary\nfrom nltk.tokenize import word_tokenize\n\nmy_documents = ['The movie was about a spaceship and aliens.',\n                'I really liked the movie!',\n                'Awesome action scenes, but boring characters.']\n\ntokenized_docs = [word_tokenize(doc.lower()) for doc in my_documents]\n\ndictionary = Dictionary(tokenized_docs)\n\ndictionary.token2id","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-06-21T03:14:02.394630Z","start_time":"2019-06-21T03:14:02.376824Z"},"trusted":true},"cell_type":"code","source":"# create a gensim corpus\ncorpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\ncorpus","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"gensim models can be easily saved, updated, and reused"},{"metadata":{},"cell_type":"markdown","source":"## Tf-idf with gensim"},{"metadata":{},"cell_type":"markdown","source":"NLP model that allows you to determine the most important words in each document. Each corpus may have shared words beyond just stopwords, and these words should be down-weighted in importance. Tf-idf ensures most common words don't show up as key words."},{"metadata":{},"cell_type":"markdown","source":"The weight will be low if the terms does appear often, because tf will then be low. However, the weight will also be low if the log is closer to zero, which means the internal equation (N/dfi) is closer to one (which means all documents contain token i)."},{"metadata":{"ExecuteTime":{"end_time":"2019-06-21T03:14:02.415378Z","start_time":"2019-06-21T03:14:02.398777Z"},"trusted":true},"cell_type":"code","source":"from gensim.models.tfidfmodel import TfidfModel\n\ntfidf = TfidfModel(corpus)\n\n# reference 2nd document\ntfidf[corpus[1]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Named-entity recognition"},{"metadata":{},"cell_type":"markdown","source":"NLP task to identify important named entities in the text, like people, places, dates, etc. "},{"metadata":{},"cell_type":"markdown","source":"nltk and the Stanford CoreNLP Library: integrated into Python via nltk, Java based, support for NER as well as conference and dependency trees. "},{"metadata":{"ExecuteTime":{"end_time":"2019-06-21T03:14:02.565239Z","start_time":"2019-06-21T03:14:02.425286Z"},"trusted":true},"cell_type":"code","source":"import nltk\nsentence = \"\"\"In New York, I like to ride the Metro to visit MOMA\n              and some restaurants rated well by Ruth Reichl.\"\"\"\n# preprocessing\ntokenized_sent = nltk.word_tokenize(sentence)\n\n# pos = part of speech\ntagged_sent = nltk.pos_tag(tokenized_sent)\n\ntagged_sent[:3]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"NNP is part of speech tag for proper noun, singular."},{"metadata":{"ExecuteTime":{"end_time":"2019-06-21T03:14:02.904606Z","start_time":"2019-06-21T03:14:02.569292Z"},"trusted":true},"cell_type":"code","source":"# chunk function, returns function as a tree\nprint(nltk.ne_chunk(tagged_sent))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Introduction to SpaCy"},{"metadata":{},"cell_type":"markdown","source":"NLP library similar to gensim, with different implementations; focus on creating NLP pipelines to generate models and corpora."},{"metadata":{"ExecuteTime":{"end_time":"2019-06-21T03:14:04.381398Z","start_time":"2019-06-21T03:14:02.915499Z"},"trusted":true},"cell_type":"code","source":"# SpaCy NER\nimport spacy\nnlp = spacy.load('en')\nnlp.entity","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-06-21T03:14:04.414982Z","start_time":"2019-06-21T03:14:04.385848Z"},"trusted":true},"cell_type":"code","source":"doc = nlp(\"\"\"Berlin is the capital of Germany; and the residence of Chancellor Angela Merkel.\"\"\")\ndoc.ents","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-06-21T03:14:04.429991Z","start_time":"2019-06-21T03:14:04.418978Z"},"trusted":true},"cell_type":"code","source":"# investigate the label for each entity\nprint(doc.ents[0], doc.ents[0].label_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Why use SpaCy for NER? Easy pipeline creation, different entity types compared to nltk, informal language corpora (easily find entities in Tweets and chat messages), quickly growing."},{"metadata":{},"cell_type":"markdown","source":"## Multilingual NER with polyglot"},{"metadata":{},"cell_type":"markdown","source":"polyglot: NLP library that uses word vectors; has support for 130+ languages"},{"metadata":{},"cell_type":"markdown","source":"Spanish NER with polyglot:"},{"metadata":{"ExecuteTime":{"end_time":"2019-06-21T03:14:04.901456Z","start_time":"2019-06-21T03:14:04.434356Z"},"trusted":true},"cell_type":"code","source":"from polyglot.text import Text\n\ntext = \"\"\"Respecto al referéndum, Puigdemont ha defendido que éste será \"efectivo\" y tendrá \"credibilidad\" si la ciudadanía \"lo hace suyo\" con una amplia movilización.\"\"\"\nptext = Text(text)\nptext.entities","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Building a \"fake news\" classifier"},{"metadata":{},"cell_type":"markdown","source":"## Classifying fake news using supervised learning with NLP"},{"metadata":{},"cell_type":"markdown","source":"Create supervised learning data from text using bag-of-words models or tf-idf as features."},{"metadata":{},"cell_type":"markdown","source":"## Building word count vectors with scikit-learn"},{"metadata":{"ExecuteTime":{"end_time":"2019-06-21T04:06:43.724710Z","start_time":"2019-06-21T04:06:42.658474Z"},"trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"df = ... # load data into DataFrame\ny = df['Sci-Fi'] # label column\nX_train, X_test, y_train, y_test = train_test_split(df['plot'], y, \n                                                    test_size=0.33,\n                                                    random_state=53)\n\n# create CountVectorizer which turns text into bow vectors\n# also remove stopwords as a preprocessing step\ncount_vectorizer = CountVectorizer(stop_words='english')\n\n# call fit_transform on the training data\ncount_train = count_vectorizer.fit_transform(X_train.values)\n\n# call transform on the test data to create bow vectors using the same dictionary\ncount_test = count_vectorizer.transform(X_test.values)"},{"metadata":{},"cell_type":"markdown","source":"We've transformed text into bag of words vectors and generated test and training dataset"},{"metadata":{},"cell_type":"markdown","source":"## Training and testing a classification model with scikit-learn"},{"metadata":{},"cell_type":"markdown","source":"Naive Bayes model: commonly used in NLP, basis in probability."},{"metadata":{},"cell_type":"markdown","source":"Example: if the plot has a spaceship, how likely is it to be sci-fi?"},{"metadata":{},"cell_type":"markdown","source":"Each word from CountVectorizer act as a feature"},{"metadata":{"ExecuteTime":{"end_time":"2019-06-21T10:44:40.879580Z","start_time":"2019-06-21T10:44:40.841637Z"},"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn import metrics","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"nb_classifier = MultinomialNB()\nnb_classifier.fit(count_train, y_train)\npred = nb_classifier.predict(count_test)\nmetrics.accuracy_score(y_test, pred)"},{"metadata":{},"cell_type":"raw","source":"# confusion matrix\nmetrics.confusion_matrix(y_test, pred, labels=[0,1])"},{"metadata":{},"cell_type":"markdown","source":"## Simple NLP, complex problems"},{"metadata":{},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false}},"nbformat":4,"nbformat_minor":1}